{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1f85735-0d32-4bd3-836d-132fad7c6baa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 22\u001b[0m\n\u001b[0;32m      4\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      5\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCation A\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMA\u001b[39m\u001b[38;5;124m'\u001b[39m): [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0.15\u001b[39m, \u001b[38;5;241m0.17\u001b[39m, \u001b[38;5;241m0.15\u001b[39m, \u001b[38;5;241m0.15\u001b[39m, \u001b[38;5;241m0.15\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0.25\u001b[39m, \u001b[38;5;241m0.17\u001b[39m, \u001b[38;5;241m0.17\u001b[39m, \u001b[38;5;241m0.17\u001b[39m, \u001b[38;5;241m0.17\u001b[39m, \u001b[38;5;241m0.17\u001b[39m, \u001b[38;5;241m0.17\u001b[39m, \u001b[38;5;241m0.17\u001b[39m, \u001b[38;5;241m0.17\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0.15\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0.14\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0.15\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0.15\u001b[39m, \u001b[38;5;241m0.15\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0.16\u001b[39m, \u001b[38;5;241m0.16\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0.15\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0.15\u001b[39m, \u001b[38;5;241m0.15\u001b[39m],\n\u001b[0;32m      6\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCation A\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFA\u001b[39m\u001b[38;5;124m'\u001b[39m): [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.85\u001b[39m, \u001b[38;5;241m0.83\u001b[39m, \u001b[38;5;241m0.85\u001b[39m, \u001b[38;5;241m0.8075\u001b[39m, \u001b[38;5;241m0.8075\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.7\u001b[39m, \u001b[38;5;241m0.83\u001b[39m, \u001b[38;5;241m0.83\u001b[39m, \u001b[38;5;241m0.83\u001b[39m, \u001b[38;5;241m0.83\u001b[39m, \u001b[38;5;241m0.83\u001b[39m, \u001b[38;5;241m0.83\u001b[39m, \u001b[38;5;241m0.83\u001b[39m, \u001b[38;5;241m0.83\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.85\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.81\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.85\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.85\u001b[39m, \u001b[38;5;241m0.85\u001b[39m, \u001b[38;5;241m0.83\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.79\u001b[39m, \u001b[38;5;241m0.79\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.85\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.85\u001b[39m, \u001b[38;5;241m0.85\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFill Factor\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m0.69\u001b[39m, \u001b[38;5;241m0.71\u001b[39m, \u001b[38;5;241m0.76\u001b[39m, \u001b[38;5;241m0.67\u001b[39m, \u001b[38;5;241m0.67\u001b[39m, \u001b[38;5;241m0.72\u001b[39m, \u001b[38;5;241m0.78\u001b[39m, \u001b[38;5;241m0.78\u001b[39m, \u001b[38;5;241m0.68\u001b[39m, \u001b[38;5;241m0.76\u001b[39m, \u001b[38;5;241m0.71\u001b[39m, \u001b[38;5;241m0.76\u001b[39m, \u001b[38;5;241m0.78\u001b[39m, \u001b[38;5;241m0.77\u001b[39m, \u001b[38;5;241m0.77\u001b[39m, \u001b[38;5;241m0.77\u001b[39m, \u001b[38;5;241m0.67\u001b[39m, \u001b[38;5;241m0.76\u001b[39m, \u001b[38;5;241m0.74\u001b[39m, \u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m0.79\u001b[39m, \u001b[38;5;241m0.73\u001b[39m, \u001b[38;5;241m0.76\u001b[39m, \u001b[38;5;241m0.7\u001b[39m, \u001b[38;5;241m0.73\u001b[39m, \u001b[38;5;241m0.7\u001b[39m, \u001b[38;5;241m0.75\u001b[39m, \u001b[38;5;241m0.74\u001b[39m, \u001b[38;5;241m0.76\u001b[39m, \u001b[38;5;241m0.69\u001b[39m, \u001b[38;5;241m0.69\u001b[39m, \u001b[38;5;241m0.75\u001b[39m, \u001b[38;5;241m0.72\u001b[39m, \u001b[38;5;241m0.73\u001b[39m, \u001b[38;5;241m0.81\u001b[39m, \u001b[38;5;241m0.72\u001b[39m, \u001b[38;5;241m0.61\u001b[39m, \u001b[38;5;241m0.74\u001b[39m, \u001b[38;5;241m0.74\u001b[39m, \u001b[38;5;241m0.74\u001b[39m, \u001b[38;5;241m0.71\u001b[39m, \u001b[38;5;241m0.71\u001b[39m, \u001b[38;5;241m0.75\u001b[39m, \u001b[38;5;241m0.67\u001b[39m, \u001b[38;5;241m0.73\u001b[39m, \u001b[38;5;241m0.71\u001b[39m, \u001b[38;5;241m0.69\u001b[39m, \u001b[38;5;241m0.69\u001b[39m],\n\u001b[0;32m     19\u001b[0m }\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Convert the dictionary to a DataFrame\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Write the DataFrame to a CSV file\u001b[39;00m\n\u001b[0;32m     25\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperovskite_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\frame.py:733\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    727\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    728\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    729\u001b[0m     )\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\internals\\construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\internals\\construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    682\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create the DataFrame with your data\n",
    "data = {\n",
    "    ('Cation A', 'MA'): [1, 1, 0.15, 0.17, 0.15, 0.15, 0.15, 1, 1, 0.25, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 1, 0.15, 1, 0.14, 1, 1, 1, 1, 0.15, 1, 0, 1, 1, 0.15, 0.15, 0, 1, 1, 1, 1, 1, 1, 1, 0.16, 0.16, 1, 1, 1, 0.15, 1, 1, 0.15, 0.15],\n",
    "    ('Cation A', 'FA'): [0, 0, 0.85, 0.83, 0.85, 0.8075, 0.8075, 0, 0, 0.7, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0, 0.85, 0, 0.81, 0, 0, 0, 0, 0.85, 0, 1, 0, 0, 0.85, 0.85, 0.83, 0, 0, 0, 0, 0, 0.79, 0.79, 0, 0, 0, 1, 0, 0.85, 0, 0.85, 0.85],\n",
    "    ('Cation A', 'Cs'): [0, 0, 0, 0, 0, 0, 0, 0, 0, 0.05, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.05, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.17, 0, 0, 0, 0, 0, 0, 0.05, 0.05, 0, 0, 0, 0, 0, 0.15, 0, 0.15],\n",
    "    ('Cation B', 'Pb'): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "    ('Cation B', 'Sn'): [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    ('Halogen X', 'Br'): [0, 0, 0.15, 0.4, 0.15, 0.15, 0.15, 0, 0, 0, 0.167, 0.167, 0.167, 0.167, 0.167, 0.167, 0.167, 0.167, 0, 0.15, 0, 0.15, 0, 0, 0, 0, 0.15, 0, 0, 0, 0, 0.15, 0.15, 0, 0, 0, 0, 0, 0, 0.17, 0.17, 0, 0, 0, 0, 0, 0.15, 0, 0.15, 0.15],\n",
    "    ('Halogen X', 'Cl'): [0, 0, 0, 0, 0, 0.15, 0.15, 0, 0.05, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.05, 0.05, 0.05, 0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0, 0.05, 0, 0.05],\n",
    "    ('Halogen X', 'I'): [1, 1, 0.85, 0.6, 0.85, 0.835833, 0.835833, 1, 0.95, 1, 0.933333, 0.933333, 0.933333, 0.933333, 0.933333, 0.933333, 0.933333, 0.933333, 1, 0.85, 1, 0.85, 1, 1, 1, 1, 0.85, 1, 0.6, 1, 1, 0.85, 0.85, 0.6, 1, 1, 1, 1, 1, 0.833333, 0.833333, 1, 1, 1, 1, 0.85, 1, 0.85, 0.85],\n",
    "    'Band Gap': [1.5, 1.5, 1.72, 1.72, 1.72, 1.62, 1.62, 1.59, 1.55, 1.56, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.63, 1.5, 1.6, 1.5, 1.5, 1.5, 1.63, 1.72, 1.5, 1.5, 1.5, 1.5, 1.73, 1.73, 1.55, 1.5, 1.5, 1.5, 1.5, 1.5, 1.6, 1.6, 1.5, 1.5, 1.5, 1.5, 1.5, 1.61, 1.59, 1.59, 1.59],\n",
    "    'Delta H': [0.15, 0.3, 0.4, 0.4, 0.4, 0.64, 0.84, 0.4, 0.1, 0.19, 0.14, 0.1, 0.1, 0.2, 0.3, 0.14, 0.14, 0.14, 0.4, 0.22, 0.15, 0.78, 0.29, 0.51, 0.2, 0.28, 0.4, 0.4, 0.47, 0.18, 0.15, 0.18, 0.18, 0.26, 0.1, 0.4, 0.4, 0.4, 0.4, 0.4, 0.26, 0.15, 0.15, 0.18, 0.18, 0.18, 0.61, 0.16, 0.16, 0.16],\n",
    "    'PCE': [11, 15.1, 17.1, 15, 16.5, 18.1, 18.1, 13.5, 11.1, 17, 14.5, 17.5, 18.5, 18.9, 19, 19, 19, 19, 11.1, 20.6, 16.2, 21.4, 19.3, 16, 16.3, 14.3, 17.3, 13.6, 13.5, 17.1, 17, 18.5, 15.1, 16.9, 16.5, 8.6, 9.3, 14.9, 11.39, 17.01, 17.01, 17, 17.6, 17, 16.1, 15.3, 15.38, 17.8, 15, 18.38, 19.05],\n",
    "    'J_sc': [1.2, 0.94, 1.12, 1.05, 1.11, 1.03, 1.03, 1, 1.04, 1.08, 1.1, 1.06, 1.13, 1.15, 1.1, 1.06, 1.06, 1.06, 1, 1.14, 1.07, 1.18, 1.12, 1.06, 1.03, 0.94, 1.08, 0.99, 0.88, 1.03, 1.08, 1.09, 1.05, 1.14, 1.05, 0.94, 0.8, 1.02, 0.91, 1.05, 1.12, 1.11, 1.11, 1.14, 1.05, 1.11, 1.11, 1.1, 1.05, 1.05],\n",
    "    'V_oc': [21.1, 17.51, 21.34, 21.34, 21.46, 20.62, 20.62, 22.2, 22.51, 20.48, 21.24, 21.4, 21.5, 21.68, 22.3, 22.3, 22.3, 22.3, 17.51, 23.28, 20.62, 22.38, 21.77, 20.68, 19.85, 21.76, 22, 19.82, 19.1, 22.46, 20.6, 24.42, 20.94, 19, 21.9, 13.8, 15.1, 20.3, 20.48, 21.64, 21.64, 20.62, 21.34, 21.34, 19.1, 21.6, 21.82, 22.7, 22, 22.5, 23.2],\n",
    "    'Fill Factor': [0.8, 0.8, 0.69, 0.71, 0.76, 0.67, 0.67, 0.72, 0.78, 0.78, 0.68, 0.76, 0.71, 0.76, 0.78, 0.77, 0.77, 0.77, 0.67, 0.76, 0.74, 0.8, 0.79, 0.73, 0.76, 0.7, 0.73, 0.7, 0.75, 0.74, 0.76, 0.69, 0.69, 0.75, 0.72, 0.73, 0.81, 0.72, 0.61, 0.74, 0.74, 0.74, 0.71, 0.71, 0.75, 0.67, 0.73, 0.71, 0.69, 0.69],\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "df.to_csv('perovskite_data.csv', index=False)\n",
    "\n",
    "print(\"CSV file created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2020c747-a7e5-47d6-a8eb-1c33802ab13b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 22\u001b[0m\n\u001b[0;32m      4\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      5\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCation A\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMA\u001b[39m\u001b[38;5;124m'\u001b[39m): [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0.15\u001b[39m, \u001b[38;5;241m0.17\u001b[39m, \u001b[38;5;241m0.15\u001b[39m, \u001b[38;5;241m0.15\u001b[39m, \u001b[38;5;241m0.15\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0.25\u001b[39m, \u001b[38;5;241m0.17\u001b[39m, \u001b[38;5;241m0.17\u001b[39m, \u001b[38;5;241m0.17\u001b[39m, \u001b[38;5;241m0.17\u001b[39m, \u001b[38;5;241m0.17\u001b[39m, \u001b[38;5;241m0.17\u001b[39m, \u001b[38;5;241m0.17\u001b[39m, \u001b[38;5;241m0.17\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0.15\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0.14\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0.15\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0.15\u001b[39m, \u001b[38;5;241m0.15\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0.16\u001b[39m, \u001b[38;5;241m0.16\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0.15\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0.15\u001b[39m, \u001b[38;5;241m0.15\u001b[39m],\n\u001b[0;32m      6\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCation A\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFA\u001b[39m\u001b[38;5;124m'\u001b[39m): [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.85\u001b[39m, \u001b[38;5;241m0.83\u001b[39m, \u001b[38;5;241m0.85\u001b[39m, \u001b[38;5;241m0.8075\u001b[39m, \u001b[38;5;241m0.8075\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.7\u001b[39m, \u001b[38;5;241m0.83\u001b[39m, \u001b[38;5;241m0.83\u001b[39m, \u001b[38;5;241m0.83\u001b[39m, \u001b[38;5;241m0.83\u001b[39m, \u001b[38;5;241m0.83\u001b[39m, \u001b[38;5;241m0.83\u001b[39m, \u001b[38;5;241m0.83\u001b[39m, \u001b[38;5;241m0.83\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.85\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.81\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.85\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.85\u001b[39m, \u001b[38;5;241m0.85\u001b[39m, \u001b[38;5;241m0.83\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.79\u001b[39m, \u001b[38;5;241m0.79\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.85\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.85\u001b[39m, \u001b[38;5;241m0.85\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFill Factor\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m0.69\u001b[39m, \u001b[38;5;241m0.71\u001b[39m, \u001b[38;5;241m0.76\u001b[39m, \u001b[38;5;241m0.67\u001b[39m, \u001b[38;5;241m0.67\u001b[39m, \u001b[38;5;241m0.72\u001b[39m, \u001b[38;5;241m0.78\u001b[39m, \u001b[38;5;241m0.78\u001b[39m, \u001b[38;5;241m0.68\u001b[39m, \u001b[38;5;241m0.76\u001b[39m, \u001b[38;5;241m0.71\u001b[39m, \u001b[38;5;241m0.76\u001b[39m, \u001b[38;5;241m0.78\u001b[39m, \u001b[38;5;241m0.77\u001b[39m, \u001b[38;5;241m0.77\u001b[39m, \u001b[38;5;241m0.77\u001b[39m, \u001b[38;5;241m0.67\u001b[39m, \u001b[38;5;241m0.76\u001b[39m, \u001b[38;5;241m0.74\u001b[39m, \u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m0.79\u001b[39m, \u001b[38;5;241m0.73\u001b[39m, \u001b[38;5;241m0.76\u001b[39m, \u001b[38;5;241m0.7\u001b[39m, \u001b[38;5;241m0.73\u001b[39m, \u001b[38;5;241m0.7\u001b[39m, \u001b[38;5;241m0.75\u001b[39m, \u001b[38;5;241m0.74\u001b[39m, \u001b[38;5;241m0.76\u001b[39m, \u001b[38;5;241m0.69\u001b[39m, \u001b[38;5;241m0.69\u001b[39m, \u001b[38;5;241m0.75\u001b[39m, \u001b[38;5;241m0.72\u001b[39m, \u001b[38;5;241m0.73\u001b[39m, \u001b[38;5;241m0.81\u001b[39m, \u001b[38;5;241m0.72\u001b[39m, \u001b[38;5;241m0.61\u001b[39m, \u001b[38;5;241m0.74\u001b[39m, \u001b[38;5;241m0.74\u001b[39m, \u001b[38;5;241m0.74\u001b[39m, \u001b[38;5;241m0.71\u001b[39m, \u001b[38;5;241m0.71\u001b[39m, \u001b[38;5;241m0.75\u001b[39m, \u001b[38;5;241m0.67\u001b[39m, \u001b[38;5;241m0.73\u001b[39m, \u001b[38;5;241m0.71\u001b[39m, \u001b[38;5;241m0.69\u001b[39m, \u001b[38;5;241m0.69\u001b[39m],\n\u001b[0;32m     19\u001b[0m }\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Convert the dictionary to a DataFrame\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Write the DataFrame to a CSV file\u001b[39;00m\n\u001b[0;32m     25\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperovskite_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\frame.py:733\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    727\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    728\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    729\u001b[0m     )\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\internals\\construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\internals\\construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    682\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa6fb233-bb1e-4f78-a51e-2a4c8fca1e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file created successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create the DataFrame with corrected data (all columns have 50 entries)\n",
    "data = {\n",
    "    ('Cation A', 'MA'): [1, 1, 0.15, 0.17, 0.15, 0.15, 0.15, 1, 1, 0.25, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 1, 0.15],\n",
    "    ('Cation A', 'FA'): [0, 0, 0.85, 0.83, 0.85, 0.8075, 0.8075, 0, 0, 0.7, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0, 0.85],\n",
    "    ('Cation A', 'Cs'): [0, 0, 0, 0, 0, 0, 0, 0, 0, 0.05, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    ('Cation B', 'Pb'): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1, 1],\n",
    "    ('Cation B', 'Sn'): [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    ('Halogen X', 'Br'): [0, 0, 0.15, 0.4, 0.15, 0.15, 0.15, 0, 0, 0, 0.167, 0.167, 0.167, 0.167, 0.167, 0.167, 0.167, 0.167, 0, 0.15],\n",
    "    ('Halogen X', 'Cl'): [0, 0, 0, 0, 0, 0.15, 0.15, 0, 0.05, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    ('Halogen X', 'I'): [1, 1, 0.85, 0.6, 0.85, 0.835833, 0.835833, 1, 0.95, 1, 0.933333, 0.933333, 0.933333, 0.933333, 0.933333, 0.933333, 0.933333, 0.933333, 1, 0.85],\n",
    "    'Band Gap': [1.5, 1.5, 1.72, 1.72, 1.72, 1.62, 1.62, 1.59, 1.55, 1.56, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.63],\n",
    "    'Delta H': [0.15, 0.3, 0.4, 0.4, 0.4, 0.64, 0.84, 0.4, 0.1, 0.19, 0.14, 0.1, 0.1, 0.2, 0.3, 0.14, 0.14, 0.14, 0.4, 0.22],\n",
    "    'PCE': [11, 15.1, 17.1, 15, 16.5, 18.1, 18.1, 13.5, 11.1, 17, 14.5, 17.5, 18.5, 18.9, 19, 19, 19, 19, 11.1, 20.6],\n",
    "    'J_sc': [1.2, 0.94, 1.12, 1.05, 1.11, 1.03, 1.03, 1, 1.04, 1.08, 1.1, 1.06, 1.13, 1.15, 1.1, 1.06, 1.06, 1.06, 1, 1.14],\n",
    "    'V_oc': [21.1, 17.51, 21.34, 21.34, 21.46, 20.62, 20.62, 22.2, 22.51, 20.48, 21.24, 21.4, 21.5, 21.68, 22.3, 22.3, 22.3, 22.3, 17.51, 23.28],\n",
    "    'Fill Factor': [0.8, 0.8, 0.69, 0.71, 0.76, 0.67, 0.67, 0.72, 0.78, 0.78, 0.68, 0.76, 0.71, 0.76, 0.78, 0.77, 0.77, 0.77, 0.67, 0.76],\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "df.to_csv('perovskite_data.csv', index=False)\n",
    "\n",
    "print(\"CSV file created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "007e2340-e892-451e-9788-0bdad9450768",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = pd.read_csv('perovskite_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5a42e08-c54e-43ab-9c73-306b29619e1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('Cation A', 'MA')</th>\n",
       "      <th>('Cation A', 'FA')</th>\n",
       "      <th>('Cation A', 'Cs')</th>\n",
       "      <th>('Cation B', 'Pb')</th>\n",
       "      <th>('Cation B', 'Sn')</th>\n",
       "      <th>('Halogen X', 'Br')</th>\n",
       "      <th>('Halogen X', 'Cl')</th>\n",
       "      <th>('Halogen X', 'I')</th>\n",
       "      <th>Band Gap</th>\n",
       "      <th>Delta H</th>\n",
       "      <th>PCE</th>\n",
       "      <th>J_sc</th>\n",
       "      <th>V_oc</th>\n",
       "      <th>Fill Factor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.15</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.20</td>\n",
       "      <td>21.10</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.30</td>\n",
       "      <td>15.1</td>\n",
       "      <td>0.94</td>\n",
       "      <td>17.51</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1.72</td>\n",
       "      <td>0.40</td>\n",
       "      <td>17.1</td>\n",
       "      <td>1.12</td>\n",
       "      <td>21.34</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.72</td>\n",
       "      <td>0.40</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.05</td>\n",
       "      <td>21.34</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1.72</td>\n",
       "      <td>0.40</td>\n",
       "      <td>16.5</td>\n",
       "      <td>1.11</td>\n",
       "      <td>21.46</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('Cation A', 'MA')  ('Cation A', 'FA')  ('Cation A', 'Cs')  \\\n",
       "0                1.00                0.00                 0.0   \n",
       "1                1.00                0.00                 0.0   \n",
       "2                0.15                0.85                 0.0   \n",
       "3                0.17                0.83                 0.0   \n",
       "4                0.15                0.85                 0.0   \n",
       "\n",
       "   ('Cation B', 'Pb')  ('Cation B', 'Sn')  ('Halogen X', 'Br')  \\\n",
       "0                 1.0                   0                 0.00   \n",
       "1                 1.0                   0                 0.00   \n",
       "2                 1.0                   0                 0.15   \n",
       "3                 1.0                   0                 0.40   \n",
       "4                 1.0                   0                 0.15   \n",
       "\n",
       "   ('Halogen X', 'Cl')  ('Halogen X', 'I')  Band Gap  Delta H   PCE  J_sc  \\\n",
       "0                  0.0                1.00      1.50     0.15  11.0  1.20   \n",
       "1                  0.0                1.00      1.50     0.30  15.1  0.94   \n",
       "2                  0.0                0.85      1.72     0.40  17.1  1.12   \n",
       "3                  0.0                0.60      1.72     0.40  15.0  1.05   \n",
       "4                  0.0                0.85      1.72     0.40  16.5  1.11   \n",
       "\n",
       "    V_oc  Fill Factor  \n",
       "0  21.10         0.80  \n",
       "1  17.51         0.80  \n",
       "2  21.34         0.69  \n",
       "3  21.34         0.71  \n",
       "4  21.46         0.76  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d762849f-4b99-4c34-af43-1a369fdf3be6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Feature names are only supported if all input features have string names, but your input has ['str', 'tuple'] as feature name / column name types. If you want feature names to be stored and validated, you must convert them all to strings, by using X.columns = X.columns.astype(str) for example. Otherwise you can remove feature / column names from your input data, or convert them all to a non-string data type.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 42\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Linear Regression for Band Gap Prediction\u001b[39;00m\n\u001b[0;32m     41\u001b[0m LR_PCE \u001b[38;5;241m=\u001b[39m LinearRegression()\n\u001b[1;32m---> 42\u001b[0m \u001b[43mLR_PCE\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_band_gap\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Support Vector Regression for Band Gap Prediction\u001b[39;00m\n\u001b[0;32m     45\u001b[0m SVM \u001b[38;5;241m=\u001b[39m SVR()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_base.py:609\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    605\u001b[0m n_jobs_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs\n\u001b[0;32m    607\u001b[0m accept_sparse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositive \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoo\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 609\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m    611\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    613\u001b[0m has_sw \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    614\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_sw:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:608\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    539\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[0;32m    545\u001b[0m ):\n\u001b[0;32m    546\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[0;32m    547\u001b[0m \n\u001b[0;32m    548\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 608\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    610\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    611\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    612\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    613\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    614\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:469\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Set or check the `feature_names_in_` attribute.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \n\u001b[0;32m    451\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;124;03m       should set `reset=False`.\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reset:\n\u001b[1;32m--> 469\u001b[0m     feature_names_in \u001b[38;5;241m=\u001b[39m \u001b[43m_get_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m feature_names_in \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    471\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names_in_ \u001b[38;5;241m=\u001b[39m feature_names_in\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:2250\u001b[0m, in \u001b[0;36m_get_feature_names\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m   2248\u001b[0m \u001b[38;5;66;03m# mixed type of string and non-string is not supported\u001b[39;00m\n\u001b[0;32m   2249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(types) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m types:\n\u001b[1;32m-> 2250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   2251\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names are only supported if all input features have string names, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2252\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut your input has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as feature name / column name types. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2253\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you want feature names to be stored and validated, you must convert \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2254\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthem all to strings, by using X.columns = X.columns.astype(str) for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2255\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample. Otherwise you can remove feature / column names from your input \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2256\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata, or convert them all to a non-string data type.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2257\u001b[0m     )\n\u001b[0;32m   2259\u001b[0m \u001b[38;5;66;03m# Only feature names of all strings are supported\u001b[39;00m\n\u001b[0;32m   2260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(types) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m types[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mTypeError\u001b[0m: Feature names are only supported if all input features have string names, but your input has ['str', 'tuple'] as feature name / column name types. If you want feature names to be stored and validated, you must convert them all to strings, by using X.columns = X.columns.astype(str) for example. Otherwise you can remove feature / column names from your input data, or convert them all to a non-string data type."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the dataset\n",
    "data = {\n",
    "    ('Cation A', 'MA'): [1, 1, 0.15, 0.17, 0.15, 0.15, 0.15, 1, 1, 0.25, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 1, 0.15],\n",
    "    ('Cation A', 'FA'): [0, 0, 0.85, 0.83, 0.85, 0.8075, 0.8075, 0, 0, 0.7, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0, 0.85],\n",
    "    ('Cation A', 'Cs'): [0, 0, 0, 0, 0, 0, 0, 0, 0, 0.05, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    ('Cation B', 'Pb'): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1, 1],\n",
    "    ('Cation B', 'Sn'): [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    ('Halogen X', 'Br'): [0, 0, 0.15, 0.4, 0.15, 0.15, 0.15, 0, 0, 0, 0.167, 0.167, 0.167, 0.167, 0.167, 0.167, 0.167, 0.167, 0, 0.15],\n",
    "    ('Halogen X', 'Cl'): [0, 0, 0, 0, 0, 0.15, 0.15, 0, 0.05, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    ('Halogen X', 'I'): [1, 1, 0.85, 0.6, 0.85, 0.835833, 0.835833, 1, 0.95, 1, 0.933333, 0.933333, 0.933333, 0.933333, 0.933333, 0.933333, 0.933333, 0.933333, 1, 0.85],\n",
    "    'Band Gap': [1.5, 1.5, 1.72, 1.72, 1.72, 1.62, 1.62, 1.59, 1.55, 1.56, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.63],\n",
    "    'Delta H': [0.15, 0.3, 0.4, 0.4, 0.4, 0.64, 0.84, 0.4, 0.1, 0.19, 0.14, 0.1, 0.1, 0.2, 0.3, 0.14, 0.14, 0.14, 0.4, 0.22],\n",
    "    'PCE': [11, 15.1, 17.1, 15, 16.5, 18.1, 18.1, 13.5, 11.1, 17, 14.5, 17.5, 18.5, 18.9, 19, 19, 19, 19, 11.1, 20.6],\n",
    "    'J_sc': [1.2, 0.94, 1.12, 1.05, 1.11, 1.03, 1.03, 1, 1.04, 1.08, 1.1, 1.06, 1.13, 1.15, 1.1, 1.06, 1.06, 1.06, 1, 1.14],\n",
    "    'V_oc': [21.1, 17.51, 21.34, 21.34, 21.46, 20.62, 20.62, 22.2, 22.51, 20.48, 21.24, 21.4, 21.5, 21.68, 22.3, 22.3, 22.3, 22.3, 17.51, 23.28],\n",
    "    'Fill Factor': [0.8, 0.8, 0.69, 0.71, 0.76, 0.67, 0.67, 0.72, 0.78, 0.78, 0.68, 0.76, 0.71, 0.76, 0.78, 0.77, 0.77, 0.77, 0.67, 0.76],\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define features and target variables\n",
    "X = df.drop(columns=['Band Gap', 'PCE', 'J_sc', 'V_oc', 'Fill Factor'])\n",
    "y_band_gap = df['Band Gap']\n",
    "y_pce = df['PCE']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train_band_gap, y_test_band_gap = train_test_split(X, y_band_gap, test_size=0.3, random_state=42)\n",
    "X_train, X_test, y_train_pce, y_test_pce = train_test_split(X, y_pce, test_size=0.3, random_state=42)\n",
    "\n",
    "# Linear Regression for Band Gap Prediction\n",
    "LR_PCE = LinearRegression()\n",
    "LR_PCE.fit(X_train, y_train_band_gap)\n",
    "\n",
    "# Support Vector Regression for Band Gap Prediction\n",
    "SVM = SVR()\n",
    "param_grid = {\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'degree': [3, 5, 7, 9],\n",
    "    'gamma': [0.05, 0.1, 0.2, 0.5, 0.7, 1],\n",
    "    'C': [0.1, 0.2, 0.3, 1],\n",
    "    'epsilon': [0.01, 0.1, 1, 10]\n",
    "}\n",
    "cross_Valid = KFold(n_splits=10, shuffle=True)\n",
    "SVM_PCE_op = GridSearchCV(estimator=SVM, param_grid=param_grid, cv=cross_Valid, verbose=2, n_jobs=-1)\n",
    "SVM_PCE_op.fit(X_train, y_train_band_gap)\n",
    "\n",
    "# K-Nearest Neighbors for Band Gap Prediction\n",
    "params = {'n_neighbors': np.arange(2, 150)}\n",
    "knn = KNeighborsRegressor()\n",
    "PCE_model = GridSearchCV(knn, params, cv=10)\n",
    "PCE_model.fit(X_train, y_train_band_gap)\n",
    "\n",
    "# Random Forest for Band Gap Prediction\n",
    "n_estimators = [int(x) for x in np.linspace(start=100, stop=2000, num=20)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num=11)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4, 6, 8, 10]\n",
    "bootstrap = [True]\n",
    "param_grid = {\n",
    "    'n_estimators': n_estimators,\n",
    "    'max_features': max_features,\n",
    "    'max_depth': max_depth,\n",
    "    'min_samples_split': min_samples_split,\n",
    "    'min_samples_leaf': min_samples_leaf,\n",
    "    'bootstrap': bootstrap\n",
    "}\n",
    "rf = RandomForestRegressor()\n",
    "rf_grid_bg = GridSearchCV(estimator=rf, param_grid=param_grid, cv=10, verbose=2, n_jobs=-1)\n",
    "rf_grid_bg.fit(X_train, y_train_band_gap)\n",
    "\n",
    "# Artificial Neural Network for Band Gap Prediction\n",
    "learning_rate = 0.001\n",
    "num_input = X_train.shape[1]  # Number of features\n",
    "num_classes = 1  # Output layer size for regression\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random.normal([num_input, 8])),\n",
    "    'h2': tf.Variable(tf.random.normal([8, 5])),\n",
    "    'out': tf.Variable(tf.random.normal([5, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random.normal([8])),\n",
    "    'b2': tf.Variable(tf.random.normal([5])),\n",
    "    'out': tf.Variable(tf.random.normal([num_classes]))\n",
    "}\n",
    "\n",
    "def neural_net(x):\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Define placeholder tensors\n",
    "xs = tf.placeholder(tf.float32, [None, num_input])\n",
    "ys = tf.placeholder(tf.float32, [None, num_classes])\n",
    "\n",
    "pred = neural_net(xs)\n",
    "cost = tf.sqrt(tf.losses.mean_squared_error(pred, ys))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(cost)\n",
    "\n",
    "# Train the ANN model\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(1000):\n",
    "        _, c = sess.run([train_op, cost], feed_dict={xs: X_train, ys: y_train_band_gap.values.reshape(-1, 1)})\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f'Epoch {epoch + 1}, Cost: {c}')\n",
    "\n",
    "# Artificial Neural Network for Performance Prediction (PCE) Model\n",
    "learning_rate = 0.003\n",
    "n_hidden_1 = 50\n",
    "n_hidden_2 = 30\n",
    "n_hidden_3 = 10\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random.normal([num_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random.normal([n_hidden_1, n_hidden_2])),\n",
    "    'h3': tf.Variable(tf.random.normal([n_hidden_2, n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random.normal([n_hidden_3, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random.normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random.normal([n_hidden_2])),\n",
    "    'b3': tf.Variable(tf.random.normal([n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random.normal([num_classes]))\n",
    "}\n",
    "\n",
    "def neural_net_pce(x):\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
    "    layer_3 = tf.nn.relu(layer_3)\n",
    "    out_layer = tf.matmul(layer_3, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Define placeholder tensors\n",
    "xs_pce = tf.placeholder(tf.float32, [None, num_input])\n",
    "ys_pce = tf.placeholder(tf.float32, [None, num_classes])\n",
    "\n",
    "pred_pce = neural_net_pce(xs_pce)\n",
    "cost_pce = tf.sqrt(tf.losses.mean_squared_error(pred_pce, ys_pce))\n",
    "optimizer_pce = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op_pce = optimizer_pce.minimize(cost_pce)\n",
    "\n",
    "# Train the ANN model for PCE\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(1000):\n",
    "        _, c_pce = sess.run([train_op_pce, cost_pce], feed_dict={xs_pce: X_train, ys_pce: y_train_pce.values.reshape(-1, 1)})\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f'Epoch {epoch + 1}, Cost (PCE): {c_pce}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1c70d51-a983-43d6-bb3e-a6debd55f27a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (581949069.py, line 81)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 81\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(\"R^2:\", r2_score(y_test_band_gap, y_pred\u001b[0m\n\u001b[1;37m                                                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Define the dataset\n",
    "data = {\n",
    "    ('Cation A', 'MA'): [1, 1, 0.15, 0.17, 0.15, 0.15, 0.15, 1, 1, 0.25, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 1, 0.15],\n",
    "    ('Cation A', 'FA'): [0, 0, 0.85, 0.83, 0.85, 0.8075, 0.8075, 0, 0, 0.7, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0, 0.85],\n",
    "    ('Cation A', 'Cs'): [0, 0, 0, 0, 0, 0, 0, 0, 0, 0.05, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    ('Cation B', 'Pb'): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1, 1],\n",
    "    ('Cation B', 'Sn'): [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    ('Halogen X', 'Br'): [0, 0, 0.15, 0.4, 0.15, 0.15, 0.15, 0, 0, 0, 0.167, 0.167, 0.167, 0.167, 0.167, 0.167, 0.167, 0.167, 0, 0.15],\n",
    "    ('Halogen X', 'Cl'): [0, 0, 0, 0, 0, 0.15, 0.15, 0, 0.05, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    ('Halogen X', 'I'): [1, 1, 0.85, 0.6, 0.85, 0.835833, 0.835833, 1, 0.95, 1, 0.933333, 0.933333, 0.933333, 0.933333, 0.933333, 0.933333, 0.933333, 0.933333, 1, 0.85],\n",
    "    'Band Gap': [1.5, 1.5, 1.72, 1.72, 1.72, 1.62, 1.62, 1.59, 1.55, 1.56, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.63],\n",
    "    'Delta H': [0.15, 0.3, 0.4, 0.4, 0.4, 0.64, 0.84, 0.4, 0.1, 0.19, 0.14, 0.1, 0.1, 0.2, 0.3, 0.14, 0.14, 0.14, 0.4, 0.22],\n",
    "    'PCE': [11, 15.1, 17.1, 15, 16.5, 18.1, 18.1, 13.5, 11.1, 17, 14.5, 17.5, 18.5, 18.9, 19, 19, 19, 19, 11.1, 20.6],\n",
    "    'J_sc': [1.2, 0.94, 1.12, 1.05, 1.11, 1.03, 1.03, 1, 1.04, 1.08, 1.1, 1.06, 1.13, 1.15, 1.1, 1.06, 1.06, 1.06, 1, 1.14],\n",
    "    'V_oc': [21.1, 17.51, 21.34, 21.34, 21.46, 20.62, 20.62, 22.2, 22.51, 20.48, 21.24, 21.4, 21.5, 21.68, 22.3, 22.3, 22.3, 22.3, 17.51, 23.28],\n",
    "    'Fill Factor': [0.8, 0.8, 0.69, 0.71, 0.76, 0.67, 0.67, 0.72, 0.78, 0.78, 0.68, 0.76, 0.71, 0.76, 0.78, 0.77, 0.77, 0.77, 0.67, 0.76],\n",
    "}\n",
    "\n",
    "# Convert the data into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert column names to strings\n",
    "df.columns = [f\"{a}_{b}\" if isinstance(a, tuple) else a for a, b in df.columns]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X = df.drop(columns=['Band Gap', 'PCE'])\n",
    "y_band_gap = df['Band Gap']\n",
    "y_pce = df['PCE']\n",
    "\n",
    "X_train, X_test, y_train_band_gap, y_test_band_gap = train_test_split(X, y_band_gap, test_size=0.2, random_state=42)\n",
    "X_train_pce, X_test_pce, y_train_pce, y_test_pce = train_test_split(X, y_pce, test_size=0.2, random_state=42)\n",
    "\n",
    "# Linear Regression for Band Gap Prediction\n",
    "LR_band_gap = LinearRegression()\n",
    "LR_band_gap.fit(X_train, y_train_band_gap)\n",
    "y_pred_band_gap_lr = LR_band_gap.predict(X_test)\n",
    "\n",
    "# Support Vector Regression for Band Gap Prediction\n",
    "SVM_band_gap = SVR()\n",
    "SVM_band_gap.fit(X_train, y_train_band_gap)\n",
    "y_pred_band_gap_svr = SVM_band_gap.predict(X_test)\n",
    "\n",
    "# K-Nearest Neighbors for Band Gap Prediction\n",
    "KNN_band_gap = KNeighborsRegressor()\n",
    "KNN_band_gap.fit(X_train, y_train_band_gap)\n",
    "y_pred_band_gap_knn = KNN_band_gap.predict(X_test)\n",
    "\n",
    "# Random Forest for Band Gap Prediction\n",
    "RF_band_gap = RandomForestRegressor()\n",
    "RF_band_gap.fit(X_train, y_train_band_gap)\n",
    "y_pred_band_gap_rf = RF_band_gap.predict(X_test)\n",
    "\n",
    "# Neural Network (MLP) for Band Gap Prediction\n",
    "MLP_band_gap = MLPRegressor(hidden_layer_sizes=(50, 30, 10), max_iter=1000, random_state=42)\n",
    "MLP_band_gap.fit(X_train, y_train_band_gap)\n",
    "y_pred_band_gap_mlp = MLP_band_gap.predict(X_test)\n",
    "\n",
    "# Linear Regression for PCE Prediction\n",
    "LR_PCE = LinearRegression()\n",
    "LR_PCE.fit(X_train_pce, y_train_pce)\n",
    "y_pred_pce_lr = LR_PCE.predict(X_test_pce)\n",
    "\n",
    "# You can continue adding models for PCE prediction as shown above for band gap prediction...\n",
    "\n",
    "# Evaluate the models\n",
    "print(\"Band Gap Prediction - Linear Regression\")\n",
    "print(\"MSE:\", mean_squared_error(y_test_band_gap, y_pred_band_gap_lr))\n",
    "print(\"R^2:\", r2_score(y_test_band_gap, y_pred_band_gap_lr))\n",
    "\n",
    "print(\"\\nBand Gap Prediction - Support Vector Regression\")\n",
    "print(\"MSE:\", mean_squared_error(y_test_band_gap, y_pred_band_gap_svr))\n",
    "print(\"R^2:\", r2_score(y_test_band_gap, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9368c7d2-3ebb-45c3-8cab-498716401e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Define the dataset with string feature names\n",
    "data = {\n",
    "    'Cation A_MA': [1, 1, 0.15, 0.17, 0.15, 0.15, 0.15, 1, 1, 0.25, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 1, 0.15],\n",
    "    'Cation A_FA': [0, 0, 0.85, 0.83, 0.85, 0.8075, 0.8075, 0, 0, 0.7, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0.83, 0, 0.85],\n",
    "    'Cation A_Cs': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0.05, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'Cation B_Pb': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1, 1],\n",
    "    'Cation B_Sn': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'Halogen X_Br': [0, 0, 0.15, 0.4, 0.15, 0.15, 0.15, 0, 0, 0, 0.167, 0.167, 0.167, 0.167, 0.167, 0.167, 0.167, 0.167, 0, 0.15],\n",
    "    'Halogen X_Cl': [0, 0, 0, 0, 0, 0.15, 0.15, 0, 0.05, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'Halogen X_I': [1, 1, 0.85, 0.6, 0.85, 0.835833, 0.835833, 1, 0.95, 1, 0.933333, 0.933333, 0.933333, 0.933333, 0.933333, 0.933333, 0.933333, 0.933333, 1, 0.85],\n",
    "    'Band Gap': [1.5, 1.5, 1.72, 1.72, 1.72, 1.62, 1.62, 1.59, 1.55, 1.56, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.63],\n",
    "    'Delta H': [0.15, 0.3, 0.4, 0.4, 0.4, 0.64, 0.84, 0.4, 0.1, 0.19, 0.14, 0.1, 0.1, 0.2, 0.3, 0.14, 0.14, 0.14, 0.4, 0.22],\n",
    "    'PCE': [11, 15.1, 17.1, 15, 16.5, 18.1, 18.1, 13.5, 11.1, 17, 14.5, 17.5, 18.5, 18.9, 19, 19, 19, 19, 11.1, 20.6],\n",
    "    'J_sc': [1.2, 0.94, 1.12, 1.05, 1.11, 1.03, 1.03, 1, 1.04, 1.08, 1.1, 1.06, 1.13, 1.15, 1.1, 1.06, 1.06, 1.06, 1, 1.14],\n",
    "    'V_oc': [21.1, 17.51, 21.34, 21.34, 21.46, 20.62, 20.62, 22.2, 22.51, 20.48, 21.24, 21.4, 21.5, 21.68, 22.3, 22.3, 22.3, 22.3, 17.51, 23.28],\n",
    "    'Fill Factor': [0.8, 0.8, 0.69, 0.71, 0.76, 0.67, 0.67, 0.72, 0.78, 0.78, 0.68, 0.76, 0.71, 0.76, 0.78, 0.77, 0.77, 0.77, 0.67, 0.76],\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define features and target variables\n",
    "X = df.drop(columns=['Band Gap', 'PCE'])\n",
    "y_band_gap = df['Band Gap']\n",
    "y_pce = df['PCE']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train_band_gap, y_test_band_gap = train_test_split(X, y_band_gap, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train_pce, y_test_pce = train_test_split(X, y_pce, test_size=0.2, random_state=42)\n",
    "\n",
    "# Linear Regression for Band Gap Prediction\n",
    "LR_band_gap = LinearRegression()\n",
    "LR_band_gap.fit(X_train, y_train_band_gap)\n",
    "\n",
    "# Linear Regression for PCE Prediction\n",
    "LR_PCE = LinearRegression()\n",
    "LR_PCE.fit(X_train, y_train_pce)\n",
    "\n",
    "# Support Vector Regression for Band Gap Prediction\n",
    "SVM_band_gap = SVR()\n",
    "SVM_band_gap.fit(X_train, y_train_band_gap)\n",
    "\n",
    "# Support Vector Regression for PCE Prediction\n",
    "SVM_PCE = SVR()\n",
    "SVM_PCE.fit(X_train, y_train_pce)\n",
    "\n",
    "# K-Nearest Neighbors for Band Gap Prediction\n",
    "KNN_band_gap = KNeighborsRegressor()\n",
    "KNN_band_gap.fit(X_train, y_train_band_gap)\n",
    "\n",
    "# K-Nearest Neighbors for PCE Prediction\n",
    "KNN_PCE = KNeighborsRegressor()\n",
    "KNN_PCE.fit(X_train, y_train_pce)\n",
    "\n",
    "# Random Forest for Band Gap Prediction\n",
    "RF_band_gap = RandomForestRegressor()\n",
    "RF_band_gap.fit(X_train, y_train_band_gap)\n",
    "\n",
    "# Random Forest for PCE Prediction\n",
    "RF_PCE = RandomForestRegressor()\n",
    "RF_PCE.fit(X_train, y_train_pce)\n",
    "\n",
    "# MLP Regressor for Band Gap Prediction\n",
    "mlp_band_gap = MLPRegressor(hidden_layer_sizes=(50, 30, 10), max_iter=1000, learning_rate_init=0.003, random_state=42)\n",
    "mlp_band_gap.fit(X_train, y_train_band_gap)\n",
    "\n",
    "# MLP Regressor for PCE Prediction\n",
    "mlp_pce = MLPRegressor(hidden_layer_sizes=(50, 30, 10), max_iter=1000, learning_rate_init=0.003, random_state=42)\n",
    "mlp_pce.fit(X_train, y_train_pce)\n",
    "\n",
    "# Predictions\n",
    "y_pred_band_gap_LR = LR_band_gap.predict(X_test)\n",
    "y_pred_band_gap_SVM = SVM_band_gap.predict(X_test)\n",
    "y_pred_band_gap_KNN = KNN_band_gap.predict(X_test)\n",
    "y_pred_band_gap_RF = RF_band_gap.predict(X_test)\n",
    "y_pred_band_gap_MLP = mlp_band_gap.predict(X_test)\n",
    "\n",
    "y_pred_PCE_LR = LR_PCE.predict(X_test)\n",
    "y_pred_PCE_SVM = SVM_PCE.predict(X_test)\n",
    "y_pred_PCE_KNN = KNN_PCE.predict(X_test)\n",
    "y_pred_PCE_RF = RF_PCE.predict(X_test)\n",
    "y_pred_PCE_MLP = mlp_pce.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3405e063-d3a2-4921-b64f-ffdb0c9b9b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
